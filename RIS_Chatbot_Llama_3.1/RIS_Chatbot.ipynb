{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9kB5fJQxTDk",
        "outputId": "b8018293-2633-4513-b0aa-66bb6c413832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: langchain in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.14)\n",
            "Requirement already satisfied: chromadb in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.5)\n",
            "Requirement already satisfied: gpt4all in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.2)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.12)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.0)\n",
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: langgraph in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.5)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.99)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.110.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.9.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.19.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.66.5)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (6.4.3)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.65.5)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.12.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.27.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.5.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
            "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (1.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: sniffio in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.34.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.4)\n",
            "Requirement already satisfied: sympy in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.47b0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.0.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.24.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: typing, google-search-results\n",
            "  Building wheel for typing (pyproject.toml): started\n",
            "  Building wheel for typing (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26327 sha256=c67322d2c20224c2a11a6c4ec62b73c92fd354ff92e9c9135a643df7bf95c79f\n",
            "  Stored in directory: c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\12\\98\\52\\2bffe242a9a487f00886e43b8ed8dac46456702e11a0d6abef\n",
            "  Building wheel for google-search-results (pyproject.toml): started\n",
            "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32083 sha256=d9757bd4a83a95127367c02892cdb8990c8c20422072237df9777ad9b0d9926a\n",
            "  Stored in directory: c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\0c\\47\\f5\\89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
            "Successfully built typing google-search-results\n",
            "Installing collected packages: typing, google-search-results\n",
            "Successfully installed google-search-results-2.4.2 typing-3.7.4.3\n"
          ]
        }
      ],
      "source": [
        "! pip install requests beautifulsoup4 langchain chromadb gpt4all langchain-community tiktoken typing langgraph google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YmXKgC89xiQN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'True'\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iwDCbvvWxlqk"
      },
      "outputs": [],
      "source": [
        "local_llm = 'llama3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TsO2iYzxuuC",
        "outputId": "aea5e00d-83ab-4f64-917e-77fccc3ba8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fetching content from https://radiopaedia.org/: 406 Client Error: Not Acceptable for url: https://radiopaedia.org/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "# Define the URLs you want to scrape\n",
        "urls = [\n",
        "    \"https://www.medscape.com/radiology\",\n",
        "    \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
        "    \"https://radiopaedia.org/\",\n",
        "    \"https://www.myesr.org/\",\n",
        "    \"https://www.bmj.com/specialties/radiology\"\n",
        "]\n",
        "\n",
        "# Function to scrape the content of a given URL\n",
        "def scrape_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract text content from the page\n",
        "        text = soup.get_text()\n",
        "        return Document(page_content=text, metadata={\"url\": url})\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching content from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Scrape documents from the URLs\n",
        "docs = [scrape_content(url) for url in urls if scrape_content(url) is not None]\n",
        "\n",
        "# Split documents\n",
        "text_spliter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "\n",
        "docs_list = text_spliter.split_documents(docs)\n",
        "\n",
        "# Filter out complex metadata and ensure proper document formatting\n",
        "filtered_docs = []\n",
        "for doc in docs_list:\n",
        "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
        "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
        "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
        "\n",
        "# Add to vector DB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=filtered_docs,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=GPT4AllEmbeddings()\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lkmKUAduyS_Z"
      },
      "outputs": [],
      "source": [
        "#Document grader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "#LLM\n",
        "llm = ChatOllama(\n",
        "    model=local_llm, \n",
        "    format=\"json\", \n",
        "    temperature=0,\n",
        "    model_kwargs={\n",
        "        'device': 'cuda',  # Use GPU if available\n",
        "        'max_new_tokens': 2048,\n",
        "        }\n",
        "    )\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    a document is useful to resolve a question. Give a binary score 'yes' or 'no' score to indicate\n",
        "    wheater the document is useful to resolve the question. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation.\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here is the document:\n",
        "    \\n ------- \\n\n",
        "    {document}\n",
        "    \\n ------- \\n\n",
        "    Here is the question: {question} <|eot_id|><start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variable=[\"question\",\"document\"]\n",
        ")\n",
        "#Chain\n",
        "retrival_grader = prompt | llm | JsonOutputParser()\n",
        "# question = \"what is radiogy\"\n",
        "\n",
        "#Run\n",
        "# question = \"X-ray in radiology\"\n",
        "# docs = retriever.invoke(question)\n",
        "# doc_txt = docs[1].page_content\n",
        "# print(retrival_grader.invoke({\"question\":question, \"document\":doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "95YPHsxTz2Zs"
      },
      "outputs": [],
      "source": [
        "### Genarate answer\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "    Use six sentences maximum to keep the answer concise\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variable=[\"question\",\"document\"]\n",
        ")\n",
        "\n",
        "llm = ChatOllama(model=local_llm, temperature=0.5)\n",
        "\n",
        "#Post processing\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "#Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "# generation = rag_chain.invoke({\"context\":docs, \"question\":question})\n",
        "\n",
        "#Run\n",
        "# question = \"X-ray in radioogy\"\n",
        "# docs = retriever.invoke(question)\n",
        "# generation = rag_chain.invoke({\"context\":docs, \"question\":question})\n",
        "# print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dhxGRZikIoQF"
      },
      "outputs": [],
      "source": [
        "# Search\n",
        "os.environ['TAVILY_API_KEY'] = \"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\"\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SCAyEv-rJuXK"
      },
      "outputs": [],
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "#LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temparature = 0)\n",
        "\n",
        "#Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate\n",
        "    wheater the answer is grounded in / supported by facts. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation.\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here are the facts:\n",
        "    \\n ------- \\n\n",
        "    {documents}\n",
        "    \\n ------- \\n\n",
        "    Here is the answer: {generation} <|eot_id|><start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variable=[\"question\",\"document\"]\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt | llm | JsonOutputParser()\n",
        "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ETarCIzbP5gn"
      },
      "outputs": [],
      "source": [
        "### Answer Grader\n",
        "\n",
        "#LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temparature = 0)\n",
        "\n",
        "#Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    an answer is useful to resolve a question. Give a binary score 'yes' or 'no' score to indicate\n",
        "    wheater the answer is useful to resolve the question. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation.\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here is the answer:\n",
        "    \\n ------- \\n\n",
        "    {generation}\n",
        "    \\n ------- \\n\n",
        "    Here is the question: {question} <|eot_id|><start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variable=[\"question\",\"document\"]\n",
        ")\n",
        "\n",
        "answer_grader = prompt | llm | JsonOutputParser()\n",
        "# answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NcUyEV1FS2qL"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "### State\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of our graph.\n",
        "\n",
        "  Attributes:\n",
        "    question: question\n",
        "    generation: LLM generation\n",
        "    web_search: whether to add search\n",
        "    documents: List of documents\n",
        "  \"\"\"\n",
        "  question: str\n",
        "  generation: str\n",
        "  web_search: str\n",
        "  documents: List [str]\n",
        "\n",
        "from langchain.schema import document\n",
        "\n",
        "### Nodes\n",
        "\n",
        "def retrieve(state):\n",
        "  \"\"\"\n",
        "  Retrieve documents form vectorstore\n",
        "\n",
        "  Arg:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, documents, that contains retrieved documents\n",
        "\n",
        "  \"\"\"\n",
        "  print(\"---RETRIEVE---\")\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  #Retrieval\n",
        "  documents = retriever.invoke(question)\n",
        "  return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def grade_documents(state):\n",
        "  \"\"\"\n",
        "  Determines whether the retrieved documents are relevent to the question\n",
        "  If any document is not relevant, we will set a flag to run web search\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): Filtered out irrelavent documents and update web_search state\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  #Score each doc\n",
        "  filtered_docs = []\n",
        "  web_search = \"No\"\n",
        "\n",
        "  for d in documents:\n",
        "    score = retrival_grader.invoke({\"question\": question, \"document\":d.page_content})\n",
        "    grade = score['score']\n",
        "    #Document relevent\n",
        "    if grade.lower() == \"yes\":\n",
        "      print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(d)\n",
        "    #Document not relavent\n",
        "    else:\n",
        "      print(\"---GRADE: DOCUMENT IRRELEVANT---\")\n",
        "      web_search = \"Yes\"\n",
        "      continue\n",
        "\n",
        "  return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "def generate(state):\n",
        "  \"\"\"\n",
        "  Genarate answer using RAG on retrieved documents\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, generation, that contains LLM generation\n",
        "\n",
        "  \"\"\"\n",
        "  print(\"---GENARATE---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  #RAG generation\n",
        "  generation = rag_chain.invoke({\"context\":documents, \"question\":question})\n",
        "  return {\"documents\": documents, \"question\":question, \"generation\": generation}\n",
        "\n",
        "def web_search(state):\n",
        "  \"\"\"\n",
        "  Web search based on the grade of documents\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): Appended web results to documents\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---WEB SEARCH---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  #Web search\n",
        "  docs = web_search_tool.invoke({\"query\": question})\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "  web_results = Document(page_content=web_results)\n",
        "  if documents is not None:\n",
        "    documents.append(web_results)\n",
        "  else:\n",
        "    docuemnts = [web_results]\n",
        "\n",
        "  return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "### Conditional Nodes\n",
        "\n",
        "def decide_to_generate(state):\n",
        "  \"\"\"\n",
        "  Determines whether to genarate an answer, or add web search\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decision for next node to call\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "  question = state[\"question\"]\n",
        "  web_search = state[\"web_search\"]\n",
        "  filtered_documents = state[\"documents\"]\n",
        "\n",
        "  if web_search == \"Yes\":\n",
        "    print(\"---DESION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION WEB SEARCH---\")\n",
        "    return \"websearch\"\n",
        "\n",
        "  else:\n",
        "    print(\"---DESION: GENERATE---\")\n",
        "    return \"generate\"\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "  \"\"\"\n",
        "  Determines whether or not the answer genarate is relevant to the question\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decision for next node to call\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK HALLUCINATION---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  generation = state[\"generation\"]\n",
        "\n",
        "  # Calculate 'grade' before using it\n",
        "  score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
        "  grade = score['score']\n",
        "\n",
        "  if grade == \"yes\":\n",
        "    print(\"---DECISION: generation IS GROUNDED IN DOCUMENTS---\")\n",
        "    print(\"---GRADE generation vs QUESTION---\")\n",
        "    score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "    grade = score['score']\n",
        "    if grade == \"yes\":\n",
        "      print(\"---DECISION: GEANRATION ADDRESSES QUESTION---\")\n",
        "      return \"useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: generation DOSE NOT ADDRESS QUESTION---\")\n",
        "      return \"not useful\"\n",
        "\n",
        "  else:\n",
        "    print(\"---DECISION: generation IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "    return \"not supported\"\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "#Define the nodes\n",
        "workflow.add_node(\"websearch\", web_search)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"grade_documents\", grade_documents)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jS54_xwmpCl5"
      },
      "outputs": [],
      "source": [
        "# Build graph\n",
        "# Clear existing start point and edges if they exist.\n",
        "start_node = '__start__'\n",
        "\n",
        "# If a path from `__start__` exists, remove it.\n",
        "if start_node in workflow.nodes:\n",
        "    connected_edges = list(workflow.edges.keys())\n",
        "    for edge in connected_edges:\n",
        "        if edge[0] == start_node:\n",
        "            workflow.remove_edge(edge[0], edge[1])\n",
        "    workflow.remove_node(start_node)\n",
        "\n",
        "# Set new entry point and build the workflow\n",
        "workflow.set_entry_point(\"retrieve\") # Use set_entry_point to designate the starting node.\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"websearch\": \"websearch\",\n",
        "        \"generate\": \"generate\"\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"websearch\", \"generate\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"websearch\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "5-hgKja0vKyw",
        "outputId": "6cda45c6-f6fb-4328-ee5e-99d035fdb134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "'Finished running: retrieve:'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (4.1 GiB) than is available (3.9 GiB)\"}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho won the last test match between Sri Lanka and India\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m     pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1029\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m-> 1029\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1456\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1454\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1455\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[11], line 65\u001b[0m, in \u001b[0;36mgrade_documents\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     62\u001b[0m web_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m---> 65\u001b[0m   score \u001b[38;5;241m=\u001b[39m \u001b[43mretrival_grader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m   grade \u001b[38;5;241m=\u001b[39m score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     67\u001b[0m   \u001b[38;5;66;03m#Document relevent\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:276\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    272\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    273\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    275\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    286\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    770\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    774\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    775\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    632\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    634\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    635\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    637\u001b[0m ]\n\u001b[0;32m    638\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:623\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 623\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m         )\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:845\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 845\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:286\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    264\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    294\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[0;32m    295\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:217\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    210\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    216\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    219\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:189\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    182\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    185\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[0;32m    188\u001b[0m     }\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:271\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mValueError\u001b[0m: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (4.1 GiB) than is available (3.9 GiB)\"}"
          ]
        }
      ],
      "source": [
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# Test\n",
        "from pprint import pprint\n",
        "inputs = {\"question\": \"who won the last test match between Sri Lanka and India\"}\n",
        "for output in app.stream(inputs):\n",
        "  for key, value in output.items():\n",
        "    pprint(f\"Finished running: {key}:\")\n",
        "print(value[\"generation\"])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
